{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "40594df4-52bc-45fc-9b9c-d7c896471b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--# Separação em tabelas Dimensão + Fato (camada Silver.)\n",
    "--# Verificações DO SCHEMA silver, criado no arquivo '00.config'\n",
    "--# Dados Padronizados\n",
    "--# SDC Tipo 2\n",
    "\n",
    "SHOW DATABASES\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "91f49512-5c34-47ea-8de4-4ab8412f0234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação do dataframe temporário 'staging' de preparação para a tabela delta 'dim.estabelecimento_SCD2:'\n",
    "# Dados Padronizados\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Fonte Bronze\n",
    "src = spark.table(\"ampev.bronze.estabelecimentos\")\n",
    "# -------------\n",
    "# Com uma janela lógica 'window' podemos criar e separar em grupos a coluna EstabelecimentoID, ordenado por data de ingestão.\n",
    "# Padroniza e pega a última versão por EstabelecimentoID (pela ingestão Bronze)\n",
    "w = Window.partitionBy(\"EstabelecimentoID\").orderBy(F.col(\"_bronze_ingest_ts\").desc())\n",
    "# Isso não faz shuffle físico automático como um groupBy. A função 'window' cria uma partição lógica para cálculo da função analítica dentro do dataframe. Dessa forma, o 'partitioning' é feito na memória, sem necessidade de shuffle físico. Criando assim 2 janelas de grupos, um para os inserts e outro para as atualizações. Ordenada da mais recente para a mais antiga.\n",
    "# Isso é essencial para a camada SILVER por que na camada BRONZE podem vir dados duplicados, mudanças de cadastro e atualizações de atributos.\n",
    "\n",
    "# 'stg' ou 'staging dataframe' é uma variável de execução temporária intermediária para ficar pronta pra ser escrita na camada SILVER. 'src' é a variável que recebe a tabela pronta da camada BRONZE:\n",
    "stg = (src\n",
    "  .select(\n",
    "      F.col(\"EstabelecimentoID\").cast(\"long\").alias(\"EstabelecimentoID\"),\n",
    "      F.col(\"Local\").cast(\"string\").alias(\"Local\"),\n",
    "      F.col(\"Email\").cast(\"string\").alias(\"Email\"),\n",
    "      F.col(\"Telefone\").cast(\"string\").alias(\"Telefone\"),\n",
    "      F.col(\"_ingest_ts\").alias(\"_bronze_ingest_ts\"), # Renomear a '_ingest_ts' para 'bronze_ingest_ts'\n",
    "      F.col(\"_source_file\").alias(\"_bronze_source_file\"), # Renomear a '_source_file' para 'bronze_source_file'\n",
    "  )\n",
    "  # Aqui criamos uma coluna no DF para auxiliar na geração de números com 'row_number' aplicada na janela 'w'\n",
    "  # Ou seja, o registro mais recente recebe o número 1, o segundo o número 2.\n",
    "  .withColumn(\"_rn\", F.row_number().over(w))\n",
    "  .filter(F.col(\"_rn\") == 1) # Com o 'filter' mantemos o mais recente, baseada na última ingestão.\n",
    "  .drop(\"_rn\") # Bom retirar a coluna para não poluir a tabela.\n",
    ")\n",
    "\n",
    "# Cria hash de atributos de negócio (para detectar mudança)\n",
    "#    (inclui coalesce pra evitar null quebrar comparação)\n",
    "# Dentro do 'staging dataframe' criemos uma coluna '_attr_hash' que será usado para comparar se houve mudança nos dados\n",
    "stg = (stg\n",
    "  .withColumn(\n",
    "      \"_attr_hash\",  # 'attr' é abreviação de 'attributes' ou atributos.\n",
    "      F.sha2(                   # A função criptográfica SHA-2 é oriunda de funções SQL como 'F'\n",
    "          F.concat_ws(\"||\",     # A função 'concat' oriunda do SQL também como 'F'\n",
    "# A função 'concat' irá concatenar os campos 'local||email||telefone' e irá transformá-los em um único atributo hash. Quando realizar o MERGE da camada SILVER haverá comparação com o 'attributes_hash' da dimensão, se houver diferença, significa que houve mudança nos dados.                      \n",
    "              F.coalesce(F.col(\"Local\"), F.lit(\"\")), # 'coalesce' se o valor fica NULL, vira STRING vazia com valor fixo 'lit'\n",
    "              F.coalesce(F.col(\"Email\"), F.lit(\"\")), # Sem 'coalesce' pode gerar inconsistência com o hash sem valores fixos\n",
    "              F.coalesce(F.col(\"Telefone\"), F.lit(\"\"))\n",
    "          ),256)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "201a49b0-4a7a-41fc-a6af-10ae9e63d3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resultado final do 'staging':\n",
    "\n",
    "display(stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25b82c0d-80d0-4957-9de6-36841afdb665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ordenação por estabelecimento:\n",
    "\n",
    "stg.orderBy(\"EstabelecimentoID\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "86a47c63-efb9-4610-a4d9-a396c09d1e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação da tabela dim_estabelecimentos_scd2:\n",
    "# Dimensão histórica de estabelecimentos com controle de versões (SCD Type 2).\n",
    "# A tabela Delta guarda a versão atual e versões antigas, incluindo quando cada versão começou e terminou.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ampev.silver.dim_estabelecimentos_scd2 (\n",
    "  surrogate_key BIGINT GENERATED ALWAYS AS IDENTITY,  \n",
    "  -- Chave substituta (incremental). A chave de negócio se repete em SCD2.\n",
    "  -- A surrogate_key NÃO se repete: cada versão do mesmo EstabelecimentoID ganha uma surrogate_key nova.\n",
    "  EstabelecimentoID BIGINT,  -- Chave de negócio (business key)\n",
    "  -- Atributos versionados: se algum mudar, uma nova linha será criada e a versão anterior será encerrada.\n",
    "  Local STRING,\n",
    "  Email STRING,\n",
    "  Telefone STRING,\n",
    "\n",
    "  -- Controle SCD2:\n",
    "  start_date DATE,        -- Início de validade da versão.\n",
    "  end_date DATE,          -- Fim de validade da versão (ex.: data de expiração).\n",
    "  is_current BOOLEAN,     -- TRUE = versão ativa; FALSE = versão histórica (facilita consultas sem depender só de data).\n",
    "\n",
    "  -- Colunas técnicas / auditoria:\n",
    "  _attr_hash STRING,                -- Hash dos atributos (Local, Email, Telefone) para detectar mudanças.\n",
    "  _bronze_ingest_ts TIMESTAMP,      -- Timestamp de ingestão na Bronze (rastreabilidade).\n",
    "  _bronze_source_file STRING,       -- Arquivo de origem na Bronze (rastreabilidade).\n",
    "  _silver_ts TIMESTAMP              -- Timestamp de processamento na Silver.\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela dim_estabelecimentos_scd2 validada/criada.\")\n",
    "\n",
    "# Observação:\n",
    "# Em SCD2, o EstabelecimentoID (chave de negócio) pode aparecer em várias linhas ao longo do tempo.\n",
    "# A surrogate_key identifica unicamente cada versão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa0e7a1-a5d5-4915-8ff2-f7487631927a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"dim total:\", spark.table(\"ampev.silver.dim_estabelecimentos_scd2\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35593bc-ffb5-43ca-a805-200785f4bae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A tabela ```'dim_estabelecimentos_scd2'``` quando criada, começa vazia.\n",
    "\n",
    "Dessa forma a dimensão-dataframe ```'dim_cur'``` fica vazio por todos os registros serem TRUE na coluna ```'is_current'``` ainda. \n",
    "\n",
    "Assim no left join, todo mundo do stg não encontra match, então ```'_is_new'``` = ```true``` para todos também.\n",
    "```'_is_changed'``` = false (porque não existe “antes” para comparar)\n",
    "```'chg'``` vira o “lote inicial” (initial load)\n",
    "\n",
    "> Comportamento correto do SCD2 na primeira execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b16b29e9-e97a-4879-b63d-853692d243fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação e carregamento da dimensão current:\n",
    "\n",
    "# dim_cur (\"dimensão current\"): carrega somente as versões atuais da dimensão (is_current = TRUE),\n",
    "# trazendo apenas as colunas necessárias para comparação (EstabelecimentoID e _attr_hash).\n",
    "# Isso melhora performance e reduz leitura de histórico desnecessário.\n",
    "# O retorno é uma dimensão vazia ainda para novos upserts\n",
    "\n",
    "dim_cur = (spark.table(\"ampev.silver.dim_estabelecimentos_scd2\")\n",
    "  .filter(F.col(\"is_current\") == True)\n",
    "  .select(\"EstabelecimentoID\", \"_attr_hash\")\n",
    ")\n",
    "\n",
    "display(dim_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6e4d67cf-9a67-47f6-aeb0-1530920f62bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detectar novos registros e registros alterados:\n",
    "# chg (\"changes\"): detecta INSERTS (novos IDs) e CHANGES (IDs existentes com hash diferente).\n",
    "\n",
    "chg = (stg.alias(\"s\")\n",
    "  .join(dim_cur.alias(\"d\"), on=\"EstabelecimentoID\", how=\"left\") \n",
    "  # Left join para manter todos do staging e identificar novos IDs\n",
    "  .withColumn(\"_is_new\", F.col(\"d.EstabelecimentoID\").isNull())       # Se não achou match na dimensão current => novo ID\n",
    "  .withColumn(\n",
    "      \"_is_changed\",\n",
    "      (~F.col(\"_is_new\")) & (F.col(\"s._attr_hash\") != F.col(\"d._attr_hash\"))\n",
    "      # Se não é novo e hash mudou => houve mudança em atributos\n",
    "  )\n",
    "  .filter(F.col(\"_is_new\") | F.col(\"_is_changed\"))                # Mantém apenas novos ou alterados (evita custo no MERGE)\n",
    "  .select(\"s.*\", \"_is_new\", \"_is_changed\")                        # Mantém colunas do staging + flags\n",
    ")\n",
    "\n",
    "display(chg)\n",
    "# Resumo:\n",
    "# '_is_new' = TRUE -> INSERT (primeira versão do ID, is_current=TRUE)\n",
    "# '_is_changed' = TRUE -> \"fecha\" versão atual (end_date / is_current=FALSE) e INSERT de nova versão (start_date hoje / is_current=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80a6f5d-1824-46e7-9e9e-31d1ddaace07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar view temporária (somente se houver mudanças)\n",
    "# Se não houver nada para mudar, encerra limpo.\n",
    "\n",
    "if not chg.take(1):\n",
    "    print(\"SCD2: nenhuma mudança detectada.\")\n",
    "else:\n",
    "    chg.createOrReplaceTempView(\"v_dim_estab_changes\")\n",
    "    print(\"SCD2: view temporária v_dim_estab_changes criada com changes (novos e alterados).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "01ec4bce-c207-4ad3-9d71-1756424d1dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"v_dim_estab_changes\").orderBy(\"EstabelecimentoID\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82767b9f-42d4-4c68-8dab-5a7486eb3ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE (UPDATE) na dimensão Delta Lake SCD2:\n",
    "# Fecha (expira) as versões atuais (is_current = true) somente para os IDs que tiveram mudança (_is_changed = true).\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO ampev.silver.dim_estabelecimentos_scd2 AS t\n",
    "USING (\n",
    "  SELECT EstabelecimentoID\n",
    "  FROM v_dim_estab_changes\n",
    "  WHERE _is_changed = true\n",
    ") AS s\n",
    "ON t.EstabelecimentoID = s.EstabelecimentoID\n",
    "AND t.is_current = true\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  t.end_date   = date_sub(current_date(), 1),\n",
    "  t.is_current = false,\n",
    "  t._silver_ts = current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# INSERT (APPEND): insere novas versões (novos IDs + IDs alterados).\n",
    "# Para changes: após fechar a versão antiga no MERGE, inserimos a nova versão como current.\n",
    "# Para novos: como não existe linha atual, o MERGE não atualiza nada; aqui entra a 1ª versão como current.\n",
    "\n",
    "new_rows = (\n",
    "    chg\n",
    "    .drop(\"_is_new\", \"_is_changed\")                 # Remove flags auxiliares\n",
    "    .withColumn(\"start_date\", F.current_date())     # Início de validade da nova versão\n",
    "    .withColumn(\"end_date\", F.lit(None).cast(\"date\"))  # Sem fim (versão vigente)\n",
    "    .withColumn(\"is_current\", F.lit(True))             # Marca como versão atual\n",
    "    .withColumn(\"_silver_ts\", F.current_timestamp())# Auditoria de processamento na Silver\n",
    "    .select(\n",
    "        \"EstabelecimentoID\",\"Local\",\"Email\",\"Telefone\",\n",
    "        \"start_date\",\"end_date\",\"is_current\",\n",
    "        \"_attr_hash\",\"_bronze_ingest_ts\",\"_bronze_source_file\",\"_silver_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "(new_rows.write\n",
    "  .mode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .saveAsTable(\"ampev.silver.dim_estabelecimentos_scd2\")\n",
    ")\n",
    "\n",
    "print(\"SCD2: dimensão atualizada (novos + mudanças).\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Observações:\n",
    "# - ID alterado:\n",
    "#     1) MERGE fecha a linha atual (is_current: true -> false; end_date = ontem).\n",
    "#     2) APPEND insere a nova versão (is_current = true; start_date = hoje; end_date = null).\n",
    "# - ID novo:\n",
    "#     MERGE não faz nada (não há linha atual); APPEND insere a primeira versão como current.\n",
    "#\n",
    "# Atenção:\n",
    "# - Esse padrão assume carga diária.\n",
    "# - Se rodar múltiplas vezes no mesmo dia, end_date pode ficar < start_date.\n",
    "#   Para cargas intradiárias, prefira start_ts/end_ts TIMESTAMP ou uma regra de end_date \"exclusiva\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3fb3a564-4181-45d7-b4a5-3eb5b22f29c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- # Validação do SCD2:\n",
    "-- # Ver a linha atual por estabelecimento:\n",
    "\n",
    "SELECT *\n",
    "FROM ampev.silver.dim_estabelecimentos_scd2\n",
    "WHERE is_current = true\n",
    "ORDER BY EstabelecimentoID;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0410bd3f-012f-4615-9307-0f131b6af3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ver histórico de um estabelecimento específico\n",
    "\n",
    "SELECT EstabelecimentoID, Local, Email, Telefone, start_date, end_date, is_current\n",
    "FROM ampev.silver.dim_estabelecimentos_scd2\n",
    "WHERE EstabelecimentoID = 1\n",
    "ORDER BY start_date;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3301b6d3-e968-4b94-864e-12c5bbd9fe4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Backfill: trocar NULL por 9999-12-31 (apenas versões atuais)\n",
    "\n",
    "UPDATE ampev.silver.dim_estabelecimentos_scd2\n",
    "SET end_date = DATE('9999-12-31')\n",
    "WHERE is_current = true AND end_date IS NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "df08c025-507c-46c8-82b1-4a62cc452751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Observações importantes:\n",
    "# Este SCD2 usa corte por dia (start_date = current_date(), end_date = current_date()-1).\n",
    "# É possível fazer com granularidade por timestamp (start_ts/end_ts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0e8af9-7e33-4f65-8961-d1825dc9c4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tabela Fato Pedidos: fato_pedidos \n",
    "# Tabela limpa, enriquecida e tipagem correta para a Camada Silver:\n",
    "# PedidoID & EstabelecimentoID deverá ser uma chave composta. 'data_venda' convertida para 'DATE'\n",
    "# Aplicação SCD Tipo 02:\n",
    "# ampev.silver.fato_pedidos_scd2 \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Fonte Bronze\n",
    "src = spark.table(\"ampev.bronze.pedidos\")\n",
    "\n",
    "# Padroniza tipos e pega a última versão por (PedidoID, EstabelecimentoID)\n",
    "w = Window.partitionBy(\"PedidoID\", \"EstabelecimentoID\").orderBy(F.col(\"_bronze_ingest_ts\").desc())\n",
    "\n",
    "stg = (src\n",
    "  .select(\n",
    "      F.col(\"PedidoID\").cast(\"long\").alias(\"PedidoID\"),\n",
    "      F.col(\"EstabelecimentoID\").cast(\"long\").alias(\"EstabelecimentoID\"),\n",
    "      F.col(\"Produto\").cast(\"string\").alias(\"Produto\"),\n",
    "      F.col(\"quantidade_vendida\").cast(\"long\").alias(\"quantidade_vendida\"),\n",
    "      F.col(\"Preco_Unitario\").cast(\"double\").alias(\"Preco_Unitario\"),\n",
    "      F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\").alias(\"data_venda\"),\n",
    "      F.col(\"_ingest_ts\").alias(\"_bronze_ingest_ts\"),\n",
    "      F.col(\"_source_file\").alias(\"_bronze_source_file\"),\n",
    "  )\n",
    "  .withColumn(\"_rn\", F.row_number().over(w))\n",
    "  .filter(F.col(\"_rn\") == 1)\n",
    "  .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "# Hash dos atributos (tudo que caracteriza o \"estado\" do pedido)\n",
    "stg = (stg\n",
    "  .withColumn(\n",
    "      \"_attr_hash\",\n",
    "      F.sha2(\n",
    "          F.concat_ws(\"||\",\n",
    "              F.coalesce(F.col(\"Produto\"), F.lit(\"\")),\n",
    "              F.coalesce(F.col(\"quantidade_vendida\").cast(\"string\"), F.lit(\"\")),\n",
    "              F.coalesce(F.col(\"Preco_Unitario\").cast(\"string\"), F.lit(\"\")),\n",
    "              F.coalesce(F.col(\"data_venda\").cast(\"string\"), F.lit(\"\"))\n",
    "          ),\n",
    "          256\n",
    "      )\n",
    "  )\n",
    ")\n",
    "\n",
    "# Cria tabela SCD2 se não existir:\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ampev.silver.fato_pedidos_scd2 (\n",
    "  surrogate_key BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "\n",
    "  PedidoID BIGINT,\n",
    "  EstabelecimentoID BIGINT,\n",
    "  Produto STRING,\n",
    "  quantidade_vendida BIGINT,\n",
    "  Preco_Unitario DOUBLE,\n",
    "  data_venda DATE,\n",
    "\n",
    "  start_date DATE,\n",
    "  end_date DATE,\n",
    "  is_current BOOLEAN,\n",
    "\n",
    "  _attr_hash STRING,\n",
    "  _bronze_ingest_ts TIMESTAMP,\n",
    "  _bronze_source_file STRING,\n",
    "  _silver_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Carrega somente o \"current\" para comparar hash\n",
    "cur = (spark.table(\"ampev.silver.fato_pedidos_scd2\")\n",
    "  .filter(F.col(\"is_current\") == True)\n",
    "  .select(\"PedidoID\", \"EstabelecimentoID\", \"_attr_hash\")\n",
    ")\n",
    "\n",
    "# Detecta novos e alterados com base na chave composta\n",
    "chg = (stg.alias(\"s\")\n",
    "  .join(cur.alias(\"c\"), on=[\"PedidoID\", \"EstabelecimentoID\"], how=\"left\")\n",
    "  .withColumn(\"_is_new\", F.col(\"c._attr_hash\").isNull())\n",
    "  .withColumn(\"_is_changed\", (~F.col(\"_is_new\")) & (F.col(\"s._attr_hash\") != F.col(\"c._attr_hash\")))\n",
    "  .filter(F.col(\"_is_new\") | F.col(\"_is_changed\"))\n",
    "  .select(\"s.*\", \"_is_new\", \"_is_changed\")\n",
    ")\n",
    "\n",
    "# Se não houver nada para mudar, encerra\n",
    "if not chg.take(1):\n",
    "    print(\"SCD2: nenhuma mudança detectada em ampev.silver.fato_pedidos_scd2.\")\n",
    "else:\n",
    "    chg.createOrReplaceTempView(\"v_pedidos_changes\")\n",
    "\n",
    "    # Fecha a versão atual dos que mudaram (chave composta)\n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO ampev.silver.fato_pedidos_scd2 t\n",
    "    USING (\n",
    "      SELECT PedidoID, EstabelecimentoID\n",
    "      FROM v_pedidos_changes\n",
    "      WHERE _is_changed = true\n",
    "    ) s\n",
    "    ON t.PedidoID = s.PedidoID\n",
    "   AND t.EstabelecimentoID = s.EstabelecimentoID\n",
    "   AND t.is_current = true\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.end_date   = date_sub(current_date(), 1),\n",
    "      t.is_current = false,\n",
    "      t._silver_ts = current_timestamp()\n",
    "    \"\"\")\n",
    "\n",
    "    # Insere nova versão (novos + alterados)\n",
    "    new_rows = (chg\n",
    "      .drop(\"_is_new\", \"_is_changed\")\n",
    "      .withColumn(\"start_date\", F.current_date())\n",
    "      .withColumn(\"end_date\", F.lit(None).cast(\"date\"))\n",
    "      .withColumn(\"is_current\", F.lit(True))\n",
    "      .withColumn(\"_silver_ts\", F.current_timestamp())\n",
    "      .select(\n",
    "          \"PedidoID\",\"EstabelecimentoID\",\"Produto\",\"quantidade_vendida\",\"Preco_Unitario\",\"data_venda\",\n",
    "          \"start_date\",\"end_date\",\"is_current\",\n",
    "          \"_attr_hash\",\"_bronze_ingest_ts\",\"_bronze_source_file\",\"_silver_ts\"\n",
    "      )\n",
    "    )\n",
    "\n",
    "    (new_rows.write\n",
    "      .mode(\"append\")\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"ampev.silver.fato_pedidos_scd2\")\n",
    "    )\n",
    "\n",
    "    print(\"SCD2: fato_pedidos_scd2 atualizado (novos + mudanças) com chave composta.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7aa52bda-1ad3-4b95-be02-e921a7f5d852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Validações: Current (estado atual)\n",
    "\n",
    "SELECT PedidoID, EstabelecimentoID, Produto, quantidade_vendida, Preco_Unitario, data_venda,\n",
    "       start_date, end_date, is_current\n",
    "FROM ampev.silver.fato_pedidos_scd2\n",
    "WHERE is_current = true\n",
    "ORDER BY PedidoID, EstabelecimentoID;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "021b2666-77e3-4d46-8176-81b349d2630e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Validações: Histórico de um pedido (pela chave composta)\n",
    "\n",
    "SELECT PedidoID, EstabelecimentoID, Produto, quantidade_vendida, Preco_Unitario, data_venda,\n",
    "       start_date, end_date, is_current\n",
    "FROM ampev.silver.fato_pedidos_scd2\n",
    "WHERE PedidoID = 1 AND EstabelecimentoID = 50\n",
    "ORDER BY start_date;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8100534774199895,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
