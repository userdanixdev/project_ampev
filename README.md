# project_ampev

ğŸ“Š Projeto AMPEV â€“ Pipeline de IngestÃ£o com Azure Databricks
ğŸ§  Objetivo do Projeto

Construir um pipeline de dados utilizando Azure Databricks + Unity Catalog + Auto Loader, realizando ingestÃ£o incremental de arquivos CSV para a camada Bronze dentro de uma arquitetura de Data Lakehouse.

O pipeline:

LÃª arquivos CSV depositados em um Volume

Processa incrementalmente via Auto Loader (cloudFiles)

Grava dados em tabelas Delta no Unity Catalog

MantÃ©m controle via checkpoint

Registra metadados de ingestÃ£o

Pode ser executado via Job agendado

ğŸ—ï¸ Arquitetura
Volumes (Landing Zone)
        â†“
Auto Loader (cloudFiles)
        â†“
Delta Tables (Bronze Layer - Unity Catalog)
        â†“
(Silver Layer - futura etapa)

ğŸ“ Estrutura do Volume

Volume utilizado:

/Volumes/ampev/bronze/landings


Estrutura organizada:

```
landings/
â”‚
â”œâ”€â”€ estabelecimentos/
â”œâ”€â”€ pedidos/
â”‚
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ estabelecimentos/
â”‚   â””â”€â”€ pedidos/
â”‚
â”œâ”€â”€ _schemas/
â”‚   â”œâ”€â”€ estabelecimentos/
â”‚   â””â”€â”€ pedidos/
â”‚
â””â”€â”€ _checkpoints/
    â”œâ”€â”€ estabelecimentos/
    â””â”€â”€ pedidos/
```

ğŸ“¦ Dados Ingeridos
ğŸ“„ estabelecimentos.csv

Colunas:

Local (string)

Email (string)

EstabelecimentoID (long)

Telefone (string)

ğŸ“„ pedidos.csv

Colunas:

PedidoID (long)

EstabelecimentoID (long)

Produto (string)

quantidade_vendida (long)

Preco_Unitario (double)

data_venda (string â†’ convertido posteriormente)

âš™ï¸ Tecnologias Utilizadas

Azure Databricks

Unity Catalog

Delta Lake

Auto Loader (cloudFiles)

GitHub (via Databricks Repos)

Jobs (Workflows)

ğŸ”„ EstratÃ©gia de IngestÃ£o
âœ” Auto Loader separado por entidade

Cada entidade possui:

Pasta exclusiva

SchemaLocation exclusivo

Checkpoint exclusivo

Tabela Delta exclusiva

Isso evita mistura de dados e garante isolamento.

âœ” Schema congelado (bootstrap controlado)

Os schemas foram inferidos a partir de arquivos sample e posteriormente congelados usando StructType(...), garantindo:

Controle de tipos

Estabilidade do pipeline

Evitar inferÃªncia automÃ¡tica incorreta

Permitir validaÃ§Ã£o futura de mudanÃ§as de layout

âœ” Metadados de ingestÃ£o

Durante o writeStream sÃ£o adicionadas colunas tÃ©cnicas:

_ingest_ts â†’ timestamp da ingestÃ£o

_source_file â†’ caminho do arquivo original (via _metadata.file_path)

ğŸš€ ExecuÃ§Ã£o do Pipeline

O pipeline Ã© executado via Databricks Job agendado.

ConfiguraÃ§Ã£o recomendada:

Trigger: Scheduled

FrequÃªncia: a cada 10 minutos

Trigger type: availableNow=True

Fluxo:

Arquivo Ã© colocado na pasta landing

Job executa

Auto Loader processa apenas novos arquivos

Dados sÃ£o gravados na Bronze

Job encerra

ğŸ” GovernanÃ§a (Unity Catalog)

As tabelas sÃ£o criadas em:

ampev.bronze.estabelecimentos
ampev.bronze.pedidos


ValidaÃ§Ãµes possÃ­veis:

SHOW TABLES IN ampev.bronze;
SELECT COUNT(*) FROM ampev.bronze.estabelecimentos;
DESCRIBE HISTORY ampev.bronze.estabelecimentos;

ğŸ” Auditoria do Pipeline

Foi implementado script de auditoria automÃ¡tica que valida:

ExistÃªncia da tabela

Quantidade de registros

PresenÃ§a de metadados

HistÃ³rico Delta

ExistÃªncia de checkpoint

Resultado final:

PIPELINE OK


ou

PIPELINE FAIL

ğŸ§ª Controle de ExecuÃ§Ã£o

O pipeline nÃ£o inicia se a pasta estiver vazia:

has_files(path)


Evita:

Erros de inferÃªncia

CriaÃ§Ã£o de checkpoint vazio

ExecuÃ§Ãµes desnecessÃ¡rias

ğŸ”„ Controle de Versionamento

IntegraÃ§Ã£o via Databricks Repos:

Repos/<usuario>/<repositorio>


Fluxo:

Clone do repositÃ³rio GitHub

Desenvolvimento dentro da pasta do repo

Commit & Push via UI do Databricks

Job aponta para notebook dentro do Repo

ğŸ“Œ Boas PrÃ¡ticas Aplicadas

SeparaÃ§Ã£o por entidade

Schema explÃ­cito

Uso de checkpoints dedicados

Uso de _metadata.file_path

ExecuÃ§Ã£o via Job

Estrutura padronizada de diretÃ³rios

Auditoria automatizada

ğŸ“ˆ PrÃ³ximos Passos (Roadmap)

Implementar camada Silver (join pedidos â†” estabelecimentos)

NormalizaÃ§Ã£o de tipos (converter data_venda para DateType)

Implementar validaÃ§Ã£o de qualidade (ex: quantidade_vendida > 0)

Adicionar monitoramento via alertas

Criar branch strategy (dev/main)

ğŸ¯ Status Atual

âœ… Volume criado
âœ… Estrutura organizada
âœ… Auto Loader configurado
âœ… Schema congelado
âœ… WriteStream configurado
âœ… Job configurado
âœ… IntegraÃ§Ã£o com Git funcionando

Pipeline Bronze operacional.