# project_ampev

## ðŸ“Š Projeto AMPEV â€“ Pipeline de IngestÃ£o com Azure Databricks

### ðŸ§­ Executive Summary:

Este projeto implementa um pipeline de ingestÃ£o incremental utilizando Azure Databricks, Unity Catalog e Delta Lake, seguindo a arquitetura Medallion (Bronze â†’ Silver â†’ Gold).

A camada Bronze jÃ¡ estÃ¡ totalmente operacional, com:

- IngestÃ£o incremental via Auto Loader (cloudFiles)
- GovernanÃ§a com Unity Catalog
- Controle de schema explÃ­cito
- Metadados de ingestÃ£o
- ExecuÃ§Ã£o automatizada via Jobs
- Versionamento com GitHub (Databricks Repos)

### ðŸ—ï¸ Arquitetura:
```
Volumes (Landing Zone)
        â†“
Auto Loader (cloudFiles)
        â†“
Delta Tables (Bronze Layer - Unity Catalog)
        â†“
(Silver Layer - futura etapa)
```

### ðŸ“ Estrutura do Volume:

Volume utilizado:

> /Volumes/ampev/bronze/landings

**Estrutura organizada:**

```
landings/
â”‚
â”œâ”€â”€ estabelecimentos/
â”œâ”€â”€ pedidos/
â”‚
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ estabelecimentos/
â”‚   â””â”€â”€ pedidos/
â”‚
â”œâ”€â”€ _schemas/
â”‚   â”œâ”€â”€ estabelecimentos/
â”‚   â””â”€â”€ pedidos/
â”‚
â””â”€â”€ _checkpoints/
    â”œâ”€â”€ estabelecimentos/
    â””â”€â”€ pedidos/
```

## ðŸ“¦ Dados Ingeridos
 ### ðŸ“„ estabelecimentos.csv

| Campo             | Tipo   |
| ----------------- | ------ |
| Local             | String |
| Email             | String |
| EstabelecimentoID | Long   |
| Telefone          | String |


### ðŸ“„ pedidos.csv:

| Campo              | Tipo                                |
| ------------------ | ----------------------------------- |
| PedidoID           | Long                                |
| EstabelecimentoID  | Long                                |
| Produto            | String                              |
| quantidade_vendida | Long                                |
| Preco_Unitario     | Double                              |
| data_venda         | String (futura conversÃ£o para Date) |



## âš™ï¸ Tecnologias Utilizadas:

- Azure Databricks
- Unity Catalog
- Delta Lake
- Auto Loader (cloudFiles)
- GitHub (via Databricks Repos)
- Jobs (Workflows)

## ðŸ”„ EstratÃ©gia de IngestÃ£o:

> Auto Loader separado por entidade:

- Cada entidade possui: 
        - Pasta exclusiva
        - SchemaLocation exclusivo
        - Checkpoint exclusivo
        - Tabela Delta exclusiva

```Isso evita mistura de dados e garante isolamento.```

> Schema congelado (bootstrap controlado):

Os schemas foram inferidos a partir de arquivos sample e posteriormente congelados usando StructType(...), garantindo:

- Controle de tipos
- Estabilidade do pipeline
- Evitar inferÃªncia automÃ¡tica incorreta
- Permitir validaÃ§Ã£o futura de mudanÃ§as de layout

## Metadados de ingestÃ£o:

Durante o writeStream sÃ£o adicionadas colunas tÃ©cnicas:

- ```_ingest_ts``` â†’ timestamp da ingestÃ£o
- ```_source_file``` â†’ caminho do arquivo original (via _metadata.file_path)

### ðŸš€ ExecuÃ§Ã£o do Pipeline:

> Pipeline executado via Databricks Job agendado.

```
ConfiguraÃ§Ã£o recomendada:
Trigger: Scheduled
FrequÃªncia: a cada 10 minutos
Trigger type: availableNow=True
```

> Fluxo:

- Arquivo Ã© colocado na pasta landing
- Job executa
- Auto Loader processa apenas novos arquivos
- Dados sÃ£o gravados na Bronze
- Job encerra

## ðŸ” GovernanÃ§a (Unity Catalog)

As tabelas sÃ£o criadas em:

 - ampev.bronze.estabelecimentos
 - ampev.bronze.pedidos

## ðŸ” Auditoria do Pipeline:

Foi implementado script de auditoria automÃ¡tica para validaÃ§Ã£o de:

- ExistÃªncia da tabela
- Quantidade de registros
- PresenÃ§a de metadados
- HistÃ³rico Delta
- ExistÃªncia de checkpoint


## ðŸ§ª Controle de ExecuÃ§Ã£o:

O pipeline nÃ£o inicia se a pasta estiver vazia:

> has_files(path)

Isso evita:

- Erros de inferÃªncia
- CriaÃ§Ã£o de checkpoint vazio
- ExecuÃ§Ãµes desnecessÃ¡rias

## ðŸ”„ Controle de Versionamento:

IntegraÃ§Ã£o via Databricks Repos:

> Repos/<usuario>/<repositorio>


Fluxo:

- Clone do repositÃ³rio GitHub
- Commit & Push via UI do Databricks


## ðŸ“Œ Boas PrÃ¡ticas Aplicadas:

- SeparaÃ§Ã£o por entidade
- Schema explÃ­cito
- Uso de checkpoints dedicados
- Uso de _metadata.file_path
- ExecuÃ§Ã£o via Job
- Estrutura padronizada de diretÃ³rios
- Auditoria automatizada

## ðŸ“ˆ PrÃ³ximos Passos (Roadmap):

- Implementar camada Silver (join pedidos â†” estabelecimentos)
- NormalizaÃ§Ã£o de tipos (converter data_venda para DateType)
- Implementar validaÃ§Ã£o de qualidade (ex: quantidade_vendida > 0)
- Adicionar monitoramento via alertas
- Criar branch strategy (dev/main)

## ðŸŽ¯ Status Atual:

- âœ… Volume criado
- âœ… Estrutura organizada
- âœ… Auto Loader configurado
- âœ… Schema congelado
- âœ… WriteStream configurado
- âœ… Job configurado
- âœ… IntegraÃ§Ã£o com Git funcionando

Pipeline Bronze operacional.