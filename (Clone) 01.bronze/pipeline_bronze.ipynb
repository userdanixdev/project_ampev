{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d551e234-eba4-4129-835e-38f9f2481406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ingestão Bronze:\n",
    "-- Azure Databricks + Unity Catalog + Volumes com as duas ingestões separadas.\n",
    "-- Obs: Catálogo Criado e Schema Bronze também em Unity Catalog. No arquivo '00.config'\n",
    "-- Criação de pastas volume no 'databricks'\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS ampev.bronze.landings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ef512ab8-9510-4a0a-9c11-1920c41777e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão Bronze:\n",
    "# Azure Databricks + Unity Catalog + Volumes com as duas ingestões separadas.\n",
    "\n",
    "BASE = \"/Volumes/ampev/bronze/landings\"  \n",
    "\n",
    "dbutils.fs.mkdirs(BASE)\n",
    "dbutils.fs.mkdirs(f\"{BASE}/estabelecimentos\")\n",
    "dbutils.fs.mkdirs(f\"{BASE}/pedidos\")\n",
    "\n",
    "display(dbutils.fs.ls(BASE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c3e8f8f-f21a-4b36-9b98-93551b4057a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  “Bootstrap” controlado: pasta samples\n",
    "\n",
    "BASE = \"/Volumes/ampev/bronze/landings\"\n",
    "SAMPLES = f\"{BASE}/samples\"\n",
    "\n",
    "dbutils.fs.mkdirs(SAMPLES)\n",
    "dbutils.fs.mkdirs(f\"{SAMPLES}/estabelecimentos\")\n",
    "dbutils.fs.mkdirs(f\"{SAMPLES}/pedidos\")\n",
    "\n",
    "display(dbutils.fs.ls(BASE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "15ba10ea-b04b-4d9b-8e36-9a400a8765fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leitura dos samples :\n",
    "\n",
    "df = spark.read.format(\"csv\").load(f\"{SAMPLES}/estabelecimentos/estabelecimentos_sample.csv\", header=True, inferSchema=True)\n",
    "display(df)\n",
    "#\n",
    "# Leitura dos samples:\n",
    "\n",
    "df_2 = spark.read.format(\"csv\").load(f\"{SAMPLES}/pedidos/pedidos_sample.csv\", header=True, inferSchema=True)\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49a64e27-7ff7-4148-a276-855d4d931690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inferir schemas apartir dos samples:\n",
    "\n",
    "schema_estab = (\n",
    "  spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"sep\",\",\")\n",
    "    .csv(f\"{SAMPLES}/estabelecimentos/*.csv\")\n",
    "    .schema\n",
    ")\n",
    "\n",
    "schema_ped = (\n",
    "  spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"sep\",\",\")\n",
    "    .csv(f\"{SAMPLES}/pedidos/*.csv\")\n",
    "    .schema\n",
    ")\n",
    "\n",
    "print(f\"Schema estabelecimentos: {schema_estab}\")\n",
    "print(f\"Schema pedidos: {schema_ped}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "69abbbf6-fc2d-4f5e-8bd5-6e9b11e54601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader separado por arquivos: estabelecimentos.csv\n",
    "\n",
    "LANDING = \"/Volumes/ampev/bronze/landings\"\n",
    "\n",
    "df_estab = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"cloudFiles.schemaLocation\", f\"{LANDING}/_schemas/estabelecimentos\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schema_estab)\n",
    "  .load(f\"{LANDING}/estabelecimentos\")\n",
    ")\n",
    "   \n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b80a4cc-b0c5-4bb7-bfe1-3ff3ea17d7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader separado: pedidos\n",
    "\n",
    "df_ped = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"cloudFiles.schemaLocation\", f\"{LANDING}/_schemas/pedidos\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schema_ped)\n",
    "  .load(f\"{LANDING}/pedidos\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c118298-8f02-4279-967a-ab7427547231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Obs: O Landing pode ficar vazia, não depende de arquivo real e não gera erro 'CF_EMPTY_DIR_FOR_SCHEMA_INFERENCE'\n",
    "O Schema fica congelado (controle total) e evita inferência errada quando chegar arquivo grande.\n",
    "\n",
    "Os diretórios de 'checkpointLocation' só são criados quando o streaming realmente inicia e faz o primeiro commit.\n",
    "Ele nasce quando acontece o primeiro micro-batch com dados.\n",
    "Fluxo normal: O arquivo cai na pasta, Stream inicia, Auto Loader detecta arquivo e Processa micro-batch.\n",
    "Assim cria a pasta de checkpoint, metadata e tabela Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25539245-fa7f-4b9e-b966-da1c9096e6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Controle de arquivos para Streaming:\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "def has_files(path: str) -> bool:\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        # ignora pastas internas como _schemas/_checkpoints\n",
    "        data_files = [f for f in files if not f.name.startswith(\"_\")]\n",
    "        return len(data_files) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Conditional para iniciar stream do arquivo estabelecimentos:\n",
    "#     \n",
    "src_estab = f\"{LANDING}/estabelecimentos\"\n",
    "\n",
    "if has_files(src_estab):\n",
    "    print(\"Arquivos encontrados para estabelecimentos. Iniciando stream...\")\n",
    "\n",
    "    from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "    df_estab_out = (df_estab\n",
    "      .withColumn(\"_ingest_ts\", current_timestamp())\n",
    "      .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "    q_estab = (df_estab_out.writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{LANDING}/_checkpoints/estabelecimentos\")\n",
    "      .trigger(availableNow=True)\n",
    "      .toTable(\"ampev.bronze.estabelecimentos\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Nenhum arquivo encontrado para estabelecimentos. Stream não iniciado.\")\n",
    "\n",
    "# Conditional para iniciar stream do arquivo pedidos:\n",
    "#     \n",
    "src_ped = f\"{LANDING}/pedidos\"\n",
    "\n",
    "if has_files(src_ped):\n",
    "    print(\"Arquivos encontrados para pedidos. Iniciando stream...\")\n",
    "\n",
    "    from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "    df_ped_out = (df_ped\n",
    "      .withColumn(\"_ingest_ts\", current_timestamp())\n",
    "      .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "    q_ped = (df_ped_out.writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{LANDING}/_checkpoints/pedidos\")\n",
    "      .trigger(availableNow=True)\n",
    "      .toTable(\"ampev.bronze.pedidos\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Nenhum arquivo encontrado para pedidos. Stream não iniciado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152e2b57-3230-47e3-8e09-f37cb00df51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dessa forma isso garante em não gerar erro de inferência, não cria checkpoint vazio, não inicia stream desnecessário.\n",
    "Assim, funciona perfeitamente com Job agendado e continua incremental quando arquivos chegarem.\n",
    "\n",
    "Recomandações para produção:\n",
    "\n",
    "- Agende esse notebook como Job (ex: a cada 5 minutos)\n",
    "- Se não houver arquivos → termina em segundos\n",
    "- Se houver arquivos → processa backlog e encerra\n",
    "\n",
    "E o checkpoint garante que nunca reprocessa o mesmo arquivo."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5958273650311081,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
